{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b26564dd-6326-4419-9c83-7c1e76a06ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9012a0e8-1cd9-4c95-a7b2-71545a806612",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "402dd849-1650-4184-8f1c-fa378b0ea303",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electricity Price Per Unit: [4, 5, 3, 5, 5, 2, 3, 3, 3, 5, 4, 3, 10, 7, 9, 7, 9, 10, 6, 9, 7, 10, 9, 6]\n",
      "Solar Power Generation: [ 0.  0.  0.  0.  0.  0.  1. 10.  3.  7.  4.  9.  3.  5.  3.  7.  5.  9.\n",
      "  0.  0.  0.  0.  0.  0.]\n",
      "Electricity Demand: [ 9  4  6 11  4 11 13 16 13 15 15 18 15 18 15 16 19 18  5  3  6  4 10  6]\n"
     ]
    }
   ],
   "source": [
    "# Electricity price per unit for each hour (higher price during peak hours)\n",
    "electricity_price_per_unit = np.random.randint(1, 6, size=12).tolist() + np.random.randint(6, 11, size=12).tolist()\n",
    "\n",
    "# Solar power generation (0 at night, 1-10 units during the day)\n",
    "solar_power_generation = np.concatenate([np.zeros(6), np.random.randint(1, 11, size=12), np.zeros(6)])\n",
    "\n",
    "# Electricity load profile (higher demand during day/evening, lower at night)\n",
    "electricity_demand = np.concatenate([np.random.randint(3, 12, size=6), np.random.randint(12, 20, size=12), np.random.randint(3, 12, size=6)])\n",
    "\n",
    "# Print arrays to verify\n",
    "print(\"Electricity Price Per Unit:\", electricity_price_per_unit)\n",
    "print(\"Solar Power Generation:\", solar_power_generation)\n",
    "print(\"Electricity Demand:\", electricity_demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9326f40b-742d-4a56-bf8a-f8025758d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = 0\n",
    "init_battery = 30\n",
    "min_battery = 0\n",
    "max_battery = 50\n",
    "action_dim = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7bc35e91-4ce0-49c7-b139-6d616a016a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Q_online network for binary classification\n",
    "Q_online = Sequential([\n",
    "    Input(shape=(2,)),                          # Input layer with shape (24,)\n",
    "    Dense(8, activation='relu', kernel_regularizer=l2(0.01)),  # Hidden Layer 1 with L2 regularization\n",
    "    Dense(2, activation='linear')               # Output layer with 1 unit and sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "Q_target = Sequential([\n",
    "    Input(shape=(2,)),                          # Input layer with shape (24,)\n",
    "    Dense(8, activation='relu', kernel_regularizer=l2(0.01)),  # Hidden Layer 1 with L2 regularization\n",
    "    Dense(2, activation='linear')              # Output layer with 1 unit and sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Compile the Q_online model for Q-learning\n",
    "Q_online.compile(optimizer='adam', \n",
    "                 loss='mean_squared_error',  # MSE loss for Q-learning\n",
    "                 metrics=['mae'])  # Mean Absolute Error can also be used to monitor learning\n",
    "\n",
    "# Compile the Q_online model for Q-learning\n",
    "Q_target.compile(optimizer='adam', \n",
    "                 loss='mean_squared_error',  # MSE loss for Q-learning\n",
    "                 metrics=['mae'])  # Mean Absolute Error can also be used to monitor learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ab96293-4311-496d-960b-8616385b5e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 1\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n",
    "target_update_freq = 1000\n",
    "buffer_size = 10000\n",
    "learning_rate = 0.001\n",
    "replay_buffer = deque(maxlen=buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7109269d-49f5-4bf3-9a07-4246b55ffc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q_target.set_weights(Q_online.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "613c38b4-a908-4d81-86d4-9c43f263be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q_target.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44145b99-762a-4cca-adc0-69afa99d2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q_online.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef09c52f-4564-48fb-92cf-264bf9b4a189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([0,1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ca26e8c-5e00-46ad-b3f5-3882ddca30d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.5924516e-08]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0e98dd38-df8e-4d22-bfcf-b6ba1388fca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "03c75552-b74b-4e1e-ad33-3fb6a7567a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour 0, State: [ 0 21], Reward: 0\n",
      "Hour 1, State: [ 1 21], Reward: -20\n",
      "Hour 2, State: [ 2 15], Reward: -20\n",
      "Hour 3, State: [ 3 15], Reward: -75\n",
      "Hour 4, State: [ 4 15], Reward: -95\n",
      "Hour 5, State: [5 4], Reward: -95\n",
      "Hour 6, State: [6 5], Reward: -134\n",
      "Hour 7, State: [7 0], Reward: -167\n",
      "Hour 8, State: [8 3], Reward: -206\n",
      "Hour 9, State: [9 0], Reward: -266\n",
      "Hour 10, State: [10  0], Reward: -326\n",
      "Hour 11, State: [11  0], Reward: -380\n",
      "Hour 12, State: [12  3], Reward: -530\n",
      "Hour 13, State: [13  8], Reward: -656\n",
      "Hour 14, State: [14 11], Reward: -791\n",
      "Hour 15, State: [15 18], Reward: -903\n",
      "Hour 16, State: [16 23], Reward: -1074\n",
      "Hour 17, State: [17  5], Reward: -1074\n",
      "Hour 18, State: [18  5], Reward: -1104\n",
      "Hour 19, State: [19  2], Reward: -1104\n",
      "Hour 20, State: [20  0], Reward: -1132\n",
      "Hour 21, State: [21  0], Reward: -1172\n",
      "Hour 22, State: [22  0], Reward: -1262\n",
      "Hour 23, State: [23  0], Reward: -1298\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = np.array([0, init_battery])  # Initialize state: [hour, battery_level]\n",
    "    reward = 0  # Reset reward at the start of each episode\n",
    "    while state[0] < 24:  # Loop for 24 hours (states from 0 to 23)\n",
    "        # Exploration vs Exploitation: Choose action based on epsilon-greedy strategy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice([0, 1])  # Random action (0: Charge, 1: Discharge)\n",
    "        else:\n",
    "            q_values = Q_online.predict(state[np.newaxis])  # Predict Q-values from current state\n",
    "            action = np.argmax(q_values)  # Choose the action with the highest Q-value\n",
    "\n",
    "        # Charging logic (action = 0)\n",
    "        if action == 0:  # Charge the battery\n",
    "            reward += -(electricity_price_per_unit[state[0]] * electricity_demand[state[0]])  # Cost of charging\n",
    "            state[1] += solar_power_generation[state[0]]  # Add solar power to the battery\n",
    "        else:  # Discharge the battery (action = 1)\n",
    "            if state[1] >= electricity_demand[state[0]]:  # If there's enough battery power\n",
    "                reward += 0  # No additional cost for discharging\n",
    "                state[1] -= electricity_demand[state[0]]  # Discharge battery\n",
    "            else:  # Not enough battery power to discharge\n",
    "                # If not enough energy, the agent will need to buy extra electricity from the grid\n",
    "                reward -= (electricity_price_per_unit[state[0]] * (electricity_demand[state[0]] - state[1]))  # Cost of extra electricity\n",
    "                state[1] = 0  # Battery is empty after discharge\n",
    "\n",
    "        # Ensure battery level is within valid range (0 to max_battery_capacity)\n",
    "        state[1] = np.clip(state[1], 0, max_battery)\n",
    "        print(f\"Hour {state[0]}, State: {state}, Reward: {reward}\")\n",
    "        # Increment the hour\n",
    "        state[0] += 1  # Move to the next hour\n",
    "\n",
    "        \n",
    "# Electricity Price Per Unit: [4, 5, 3, 5, 5, 2, 3, 3, 3, 5, 4, 3, 10, 7, 9, 7, 9, 10, 6, 9, 7, 10, 9, 6]\n",
    "# Solar Power Generation: [ 0.  0.  0.  0.  0.  0.  1. 10.  3.  7.  4.  9.  3.  5.  3.  7.  5.  9.\n",
    "#   0.  0.  0.  0.  0.  0.]\n",
    "# Electricity Demand: [ 9  4  6 11  4 11 13 16 13 15 15 18 15 18 15 16 19 18  5  3  6  4 10  6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81d342-d635-4a0e-8213-994f926085eb",
   "metadata": {},
   "source": [
    "Initialize environment:\n",
    "    - Define electricity price per unit, solar power generation, electricity demand, and initial battery level\n",
    "    - Set the current step to 0\n",
    "\n",
    "Initialize Q-networks:\n",
    "    - Q_online (the model being trained)\n",
    "    - Q_target (the target model)\n",
    "\n",
    "Initialize replay buffer (empty)\n",
    "\n",
    "Initialize optimizer (e.g., Adam with learning rate)\n",
    "\n",
    "Set hyperparameters:\n",
    "    - gamma = discount factor\n",
    "    - epsilon = exploration rate (starting value)\n",
    "    - epsilon_min = minimum epsilon value\n",
    "    - epsilon_decay = epsilon decay factor\n",
    "    - num_episodes = number of episodes to train the model\n",
    "    - batch_size = size of the batch sampled from the replay buffer\n",
    "    - target_update_freq = frequency of updating the target network\n",
    "\n",
    "Main Training Loop (for each episode):\n",
    "    Reset environment\n",
    "    - Set initial state (price, battery level)\n",
    "    - Initialize total episode reward = 0\n",
    "\n",
    "    While not done (until 24 steps):\n",
    "        1. Select action (0 = charge, 1 = discharge):\n",
    "            - With probability epsilon, select a random action (explore)\n",
    "            - Otherwise, select the action with the highest Q-value (exploit) based on the current state\n",
    "\n",
    "        2. Execute the action:\n",
    "            - Apply the action in the environment (charge/discharge solar)\n",
    "            - Get the reward (based on battery state and electricity price)\n",
    "            - Get the next state (price, updated battery level)\n",
    "            - Check if the episode is done (24 steps reached)\n",
    "\n",
    "        3. Store the transition (state, action, reward, next_state, done) in the replay buffer\n",
    "\n",
    "        4. Sample a minibatch of transitions from the replay buffer (size = batch_size)\n",
    "\n",
    "        5. For each transition in the minibatch:\n",
    "            - Calculate the target Q-value using the Bellman equation:\n",
    "                Q_target = reward + gamma * max(Q_next_state) if not done else reward\n",
    "\n",
    "        6. Perform a gradient descent step on Q_online:\n",
    "            - Use the loss function (Mean Squared Error) between predicted Q-values and target Q-values\n",
    "            - Update Q_online weights\n",
    "\n",
    "        7. Periodically update the target network (Q_target) with Q_online weights\n",
    "\n",
    "        8. Decay epsilon (epsilon = max(epsilon_min, epsilon * epsilon_decay))\n",
    "\n",
    "    Print the total reward for the episode\n",
    "\n",
    "    After all episodes:\n",
    "        - Q_online model is trained to minimize the cost of electricity usage by optimizing charging/discharging decisions\n",
    "\n",
    "End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "32cfdf46-b4d8-4d09-b9a0-e10f79ae372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Reward: -1390, Epsilon: 0.995\n",
      "Episode 2/100, Reward: -1295, Epsilon: 0.990025\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Episode 3/100, Reward: -1350, Epsilon: 0.985074875\n",
      "Episode 4/100, Reward: -1213, Epsilon: 0.9801495006250001\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Episode 5/100, Reward: -1177, Epsilon: 0.9752487531218751\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 6/100, Reward: -1297, Epsilon: 0.9703725093562657\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Episode 7/100, Reward: -1225, Epsilon: 0.9655206468094844\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Episode 8/100, Reward: -1321, Epsilon: 0.960693043575437\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Episode 9/100, Reward: -1366, Epsilon: 0.9558895783575597\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 10/100, Reward: -1342, Epsilon: 0.9511101304657719\n",
      "Episode 11/100, Reward: -1423, Epsilon: 0.946354579813443\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Episode 12/100, Reward: -1321, Epsilon: 0.9416228069143757\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Episode 13/100, Reward: -1223, Epsilon: 0.9369146928798039\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Episode 14/100, Reward: -1178, Epsilon: 0.9322301194154049\n",
      "Episode 15/100, Reward: -1482, Epsilon: 0.9275689688183278\n",
      "Episode 16/100, Reward: -1380, Epsilon: 0.9229311239742362\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 119\u001b[0m\n\u001b[0;32m    117\u001b[0m         minibatch \u001b[38;5;241m=\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39msample(batch_size)\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m state_batch, action_batch, reward_batch, next_state_batch \u001b[38;5;129;01min\u001b[39;00m minibatch:\n\u001b[1;32m--> 119\u001b[0m             update_q_values(state_batch, action_batch, reward_batch, next_state_batch)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Epsilon decay\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epsilon \u001b[38;5;241m>\u001b[39m epsilon_min:\n",
      "Cell \u001b[1;32mIn[97], line 72\u001b[0m, in \u001b[0;36mupdate_q_values\u001b[1;34m(state, action, reward, next_state)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Compute gradients and apply them to the Q-online network\u001b[39;00m\n\u001b[0;32m     71\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, Q_online\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m---> 72\u001b[0m Q_online_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, Q_online\u001b[38;5;241m.\u001b[39mtrainable_variables))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:344\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[0;32m    343\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(grads, trainable_variables)\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterations\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:409\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[1;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[0;32m    406\u001b[0m     grads \u001b[38;5;241m=\u001b[39m [g \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m g \u001b[38;5;241m/\u001b[39m scale \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads]\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Apply gradient updates.\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend_apply_gradients(grads, trainable_variables)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:472\u001b[0m, in \u001b[0;36mBaseOptimizer._backend_apply_gradients\u001b[1;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;66;03m# Run udpate step.\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend_update_step(\n\u001b[0;32m    473\u001b[0m         grads, trainable_variables, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[0;32m    474\u001b[0m     )\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_variables_moving_average(\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable_variables\n\u001b[0;32m    479\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:122\u001b[0m, in \u001b[0;36mTFOptimizer._backend_update_step\u001b[1;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[0;32m    120\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[0;32m    121\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_reduce_sum_gradients(grads_and_vars)\n\u001b[1;32m--> 122\u001b[0m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39minterim\u001b[38;5;241m.\u001b[39mmaybe_merge_call(\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distributed_tf_update_step,\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy,\n\u001b[0;32m    125\u001b[0m     grads_and_vars,\n\u001b[0;32m    126\u001b[0m     learning_rate,\n\u001b[0;32m    127\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[1;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(strategy, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[0;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:136\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step\u001b[1;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(grad, var, learning_rate)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[1;32m--> 136\u001b[0m     distribution\u001b[38;5;241m.\u001b[39mextended\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    137\u001b[0m         var,\n\u001b[0;32m    138\u001b[0m         apply_grad_to_update_var,\n\u001b[0;32m    139\u001b[0m         args\u001b[38;5;241m=\u001b[39m(grad, learning_rate),\n\u001b[0;32m    140\u001b[0m         group\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    141\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3005\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3002\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m   3003\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3004\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m-> 3005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update(var, fn, args, kwargs, group)\n\u001b[0;32m   3006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3007\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[0;32m   3008\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4075\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   4072\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[0;32m   4073\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[0;32m   4074\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[1;32m-> 4075\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_non_slot(var, fn, (var,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(args), kwargs, group)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4081\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   4077\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[0;32m   4078\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[0;32m   4079\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[0;32m   4080\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[1;32m-> 4081\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4082\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[0;32m   4083\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[1;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:133\u001b[0m, in \u001b[0;36mTFOptimizer._distributed_tf_update_step.<locals>.apply_grad_to_update_var\u001b[1;34m(var, grad, learning_rate)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_grad_to_update_var\u001b[39m(var, grad, learning_rate):\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(grad, var, learning_rate)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py:145\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[1;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign(v_hat, ops\u001b[38;5;241m.\u001b[39mmaximum(v_hat, v))\n\u001b[0;32m    144\u001b[0m     v \u001b[38;5;241m=\u001b[39m v_hat\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massign_sub(\n\u001b[0;32m    146\u001b[0m     variable,\n\u001b[0;32m    147\u001b[0m     ops\u001b[38;5;241m.\u001b[39mdivide(\n\u001b[0;32m    148\u001b[0m         ops\u001b[38;5;241m.\u001b[39mmultiply(m, alpha), ops\u001b[38;5;241m.\u001b[39madd(ops\u001b[38;5;241m.\u001b[39msqrt(v), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon)\n\u001b[0;32m    149\u001b[0m     ),\n\u001b[0;32m    150\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:74\u001b[0m, in \u001b[0;36mTFOptimizer.assign_sub\u001b[1;34m(self, variable, value)\u001b[0m\n\u001b[0;32m     72\u001b[0m     variable\u001b[38;5;241m.\u001b[39mscatter_sub(value)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     variable\u001b[38;5;241m.\u001b[39massign_sub(value)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1017\u001b[0m, in \u001b[0;36mBaseResourceVariable.assign_sub\u001b[1;34m(self, delta, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;66;03m# TODO(apassos): this here and below is not atomic. Consider making it\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# atomic if there's a way to do so without a performance cost for those who\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;66;03m# don't need it.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _handle_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dependencies():\n\u001b[1;32m-> 1017\u001b[0m   assign_sub_op \u001b[38;5;241m=\u001b[39m gen_resource_variable_ops\u001b[38;5;241m.\u001b[39massign_sub_variable_op(\n\u001b[0;32m   1018\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   1019\u001b[0m       ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(delta, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype),\n\u001b[0;32m   1020\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_value:\n\u001b[0;32m   1022\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_read(assign_sub_op)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:99\u001b[0m, in \u001b[0;36massign_sub_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m     98\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[0;32m    100\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssignSubVariableOp\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, resource, value)\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    102\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Hyperparameters\n",
    "num_episodes = 100\n",
    "max_battery = 100  # Maximum battery capacity\n",
    "epsilon = 1.0  # Exploration probability\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n",
    "gamma = 0.99  # Discount factor\n",
    "learning_rate = 0.001\n",
    "batch_size = 32  # Batch size for replay buffer sampling\n",
    "\n",
    "# Initialize replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=10000):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def store(self, state, action, reward, next_state):\n",
    "        # If the buffer is full, remove the oldest transition\n",
    "        if len(self.buffer) >= self.max_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append((state, action, reward, next_state))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        # Randomly sample a batch of transitions from the buffer\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialize Q-network (online and target networks)\n",
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.dense1 = layers.Dense(8, activation='relu')\n",
    "        self.dense2 = layers.Dense(8, activation='relu')\n",
    "        self.dense3 = layers.Dense(2)  # Output layer for two actions (charge or discharge)\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "# Initialize Q-network (online and target networks)\n",
    "Q_online = QNetwork()\n",
    "Q_target = QNetwork()  # Target network\n",
    "Q_online_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Function to update Q-values using the Q-learning algorithm\n",
    "def update_q_values(state, action, reward, next_state):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict Q-values for the current state using Q_online\n",
    "        q_values = Q_online(state[np.newaxis])  # Shape: (1, 2) for two actions\n",
    "        q_value = q_values[0, action]\n",
    "\n",
    "        # Predict Q-values for the next state using Q_target\n",
    "        next_q_values = Q_target(next_state[np.newaxis])  # Shape: (1, 2)\n",
    "        next_q_value = np.max(next_q_values)  # Max Q-value for next state\n",
    "        \n",
    "        # Calculate the target Q-value\n",
    "        target = reward + gamma * next_q_value\n",
    "\n",
    "        # Compute the loss (mean squared error)\n",
    "        loss = tf.reduce_mean(tf.square(target - q_value))\n",
    "\n",
    "    # Compute gradients and apply them to the Q-online network\n",
    "    grads = tape.gradient(loss, Q_online.trainable_variables)\n",
    "    Q_online_optimizer.apply_gradients(zip(grads, Q_online.trainable_variables))\n",
    "\n",
    "# Function for soft update of Q_target network\n",
    "def update_target_network():\n",
    "    Q_target.set_weights(Q_online.get_weights())\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = ReplayBuffer(max_size=10000)\n",
    "\n",
    "# Main training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = np.array([0, init_battery])  # Initialize state: [hour, battery_level]\n",
    "    reward = 0  # Reset reward at the start of each episode\n",
    "    while state[0] < 24:  # Loop for 24 hours (states from 0 to 23)\n",
    "        # Exploration vs Exploitation: Choose action based on epsilon-greedy strategy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice([0, 1])  # Random action (0: Charge, 1: Discharge)\n",
    "        else:\n",
    "            q_values = Q_online.predict(state[np.newaxis])  # Predict Q-values from current state\n",
    "            action = np.argmax(q_values)  # Choose the action with the highest Q-value\n",
    "\n",
    "        # Charging logic (action = 0)\n",
    "        if action == 0:  # Charge the battery\n",
    "            reward += -(electricity_price_per_unit[state[0]] * electricity_demand[state[0]])  # Cost of charging\n",
    "            state[1] += solar_power_generation[state[0]]  # Add solar power to the battery\n",
    "        else:  # Discharge the battery (action = 1)\n",
    "            if state[1] >= electricity_demand[state[0]]:  # If there's enough battery power\n",
    "                reward += 0  # No additional cost for discharging\n",
    "                state[1] -= electricity_demand[state[0]]  # Discharge battery\n",
    "            else:  # Not enough battery power to discharge\n",
    "                # If not enough energy, the agent will need to buy extra electricity from the grid\n",
    "                reward -= (electricity_price_per_unit[state[0]] * (electricity_demand[state[0]] - state[1]))  # Cost of extra electricity\n",
    "                state[1] = 0  # Battery is empty after discharge\n",
    "\n",
    "        # Ensure battery level is within valid range (0 to max_battery_capacity)\n",
    "        state[1] = np.clip(state[1], 0, max_battery)\n",
    "        \n",
    "        # Store experience in replay buffer\n",
    "        replay_buffer.store(state, action, reward, state)\n",
    "\n",
    "        # Increment the hour\n",
    "        state[0] += 1  # Move to the next hour\n",
    "\n",
    "        # Perform experience replay\n",
    "        if replay_buffer.size() >= batch_size:\n",
    "            minibatch = replay_buffer.sample(batch_size)\n",
    "            for state_batch, action_batch, reward_batch, next_state_batch in minibatch:\n",
    "                update_q_values(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "    # Epsilon decay\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Periodic target network update (every few episodes)\n",
    "    if episode % 10 == 0:\n",
    "        update_target_network()\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Reward: {reward}, Epsilon: {epsilon}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "72d22252-5822-4650-b4a9-3e0f1ea0a942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_battery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "804fecee-e418-4b19-be1b-3cacb1deefc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/50, Reward: -997, Epsilon: 0.995\n",
      "Episode 2/50, Reward: -1144, Epsilon: 0.990025\n",
      "Episode 3/50, Reward: -1043, Epsilon: 0.985074875\n",
      "Episode 4/50, Reward: -922, Epsilon: 0.9801495006250001\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Episode 5/50, Reward: -592, Epsilon: 0.9752487531218751\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 6/50, Reward: -664, Epsilon: 0.9703725093562657\n",
      "Episode 7/50, Reward: -1110, Epsilon: 0.9655206468094844\n",
      "Episode 8/50, Reward: -935, Epsilon: 0.960693043575437\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 9/50, Reward: -923, Epsilon: 0.9558895783575597\n",
      "Episode 10/50, Reward: -1175, Epsilon: 0.9511101304657719\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 11/50, Reward: -1042, Epsilon: 0.946354579813443\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 12/50, Reward: -755, Epsilon: 0.9416228069143757\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 13/50, Reward: -865, Epsilon: 0.9369146928798039\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 14/50, Reward: -938, Epsilon: 0.9322301194154049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Episode 15/50, Reward: -906, Epsilon: 0.9275689688183278\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Episode 16/50, Reward: -905, Epsilon: 0.9229311239742362\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 17/50, Reward: -1065, Epsilon: 0.918316468354365\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 18/50, Reward: -946, Epsilon: 0.9137248860125932\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 19/50, Reward: -981, Epsilon: 0.9091562615825302\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 20/50, Reward: -739, Epsilon: 0.9046104802746175\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 21/50, Reward: -1069, Epsilon: 0.9000874278732445\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 22/50, Reward: -858, Epsilon: 0.8955869907338783\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 23/50, Reward: -1007, Epsilon: 0.8911090557802088\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 24/50, Reward: -820, Epsilon: 0.8866535105013078\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 25/50, Reward: -613, Epsilon: 0.8822202429488013\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 26/50, Reward: -709, Epsilon: 0.8778091417340573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 27/50, Reward: -1037, Epsilon: 0.8734200960253871\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 28/50, Reward: -842, Epsilon: 0.8690529955452602\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 29/50, Reward: -595, Epsilon: 0.8647077305675338\n",
      "Episode 30/50, Reward: -820, Epsilon: 0.8603841919146962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 31/50, Reward: -961, Epsilon: 0.8560822709551227\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 32/50, Reward: -931, Epsilon: 0.851801859600347\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 33/50, Reward: -1025, Epsilon: 0.8475428503023453\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 34/50, Reward: -672, Epsilon: 0.8433051360508336\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Episode 35/50, Reward: -907, Epsilon: 0.8390886103705794\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 36/50, Reward: -721, Epsilon: 0.8348931673187264\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 37/50, Reward: -844, Epsilon: 0.8307187014821328\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 38/50, Reward: -1068, Epsilon: 0.8265651079747222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Episode 39/50, Reward: -1004, Epsilon: 0.8224322824348486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 40/50, Reward: -1020, Epsilon: 0.8183201210226743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 41/50, Reward: -972, Epsilon: 0.8142285204175609\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 42/50, Reward: -925, Epsilon: 0.810157377815473\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 43/50, Reward: -757, Epsilon: 0.8061065909263957\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 44/50, Reward: -925, Epsilon: 0.8020760579717637\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 45/50, Reward: -786, Epsilon: 0.798065677681905\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Episode 46/50, Reward: -944, Epsilon: 0.7940753492934954\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 47/50, Reward: -835, Epsilon: 0.7901049725470279\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 48/50, Reward: -954, Epsilon: 0.7861544476842928\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 49/50, Reward: -945, Epsilon: 0.7822236754458713\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Episode 50/50, Reward: -658, Epsilon: 0.778312557068642\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Hyperparameters\n",
    "num_episodes = 50  # Reduced number of episodes for faster testing\n",
    "max_battery = 100  # Maximum battery capacity\n",
    "epsilon = 1.0  # Exploration probability\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n",
    "gamma = 0.99  # Discount factor\n",
    "learning_rate = 0.005  # Increased learning rate for faster convergence\n",
    "batch_size = 16  # Reduced batch size for faster updates\n",
    "\n",
    "# Initialize replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=10000):\n",
    "        self.buffer = []\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def store(self, state, action, reward, next_state):\n",
    "        # If the buffer is full, remove the oldest transition\n",
    "        if len(self.buffer) >= self.max_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append((state, action, reward, next_state))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        # Randomly sample a batch of transitions from the buffer\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialize Q-network (online and target networks)\n",
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.dense1 = layers.Dense(4, activation='relu')  # Reduced layer size\n",
    "        self.dense2 = layers.Dense(4, activation='relu')\n",
    "        self.dense3 = layers.Dense(2)  # Output layer for two actions (charge or discharge)\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "# Initialize Q-network (online and target networks)\n",
    "Q_online = QNetwork()\n",
    "Q_target = QNetwork()  # Target network\n",
    "Q_online_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Function to update Q-values using the Q-learning algorithm\n",
    "def update_q_values(states, actions, rewards, next_states):\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = Q_online(states)  # Use batch processing directly\n",
    "        q_value = tf.reduce_sum(q_values * tf.one_hot(actions, 2), axis=1)\n",
    "\n",
    "        next_q_values = Q_target(next_states)  # Use batch processing for next states\n",
    "        next_q_value = tf.reduce_max(next_q_values, axis=1)\n",
    "\n",
    "        target = rewards + gamma * next_q_value\n",
    "        loss = tf.reduce_mean(tf.square(target - q_value))\n",
    "\n",
    "    grads = tape.gradient(loss, Q_online.trainable_variables)\n",
    "    Q_online_optimizer.apply_gradients(zip(grads, Q_online.trainable_variables))\n",
    "\n",
    "# Function for soft update of Q_target network\n",
    "def update_target_network():\n",
    "    Q_target.set_weights(Q_online.get_weights())\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = ReplayBuffer(max_size=10000)\n",
    "\n",
    "# Simulated environment parameters (Replace with actual data)\n",
    "electricity_price_per_unit = np.random.randint(1, 6, size=12).tolist() + np.random.randint(6, 11, size=12).tolist()\n",
    "\n",
    "# Solar power generation (0 at night, 1-10 units during the day)\n",
    "solar_power_generation = np.concatenate([np.zeros(6), np.random.randint(1, 11, size=12), np.zeros(6)])\n",
    "\n",
    "# Electricity load profile (higher demand during day/evening, lower at night)\n",
    "electricity_demand = np.concatenate([np.random.randint(3, 12, size=6), np.random.randint(12, 20, size=12), np.random.randint(3, 12, size=6)])\n",
    "\n",
    "# Main training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = np.array([0, max_battery])  # Initialize state: [hour, battery_level]\n",
    "    reward = 0  # Reset reward at the start of each episode\n",
    "    while state[0] < 24:\n",
    "        # Exploration vs Exploitation: Choose action based on epsilon-greedy strategy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice([0, 1])  # Random action (0: Charge, 1: Discharge)\n",
    "        else:\n",
    "            q_values = Q_online.predict(state[np.newaxis])  # Predict Q-values from current state\n",
    "            action = np.argmax(q_values)  # Choose the action with the highest Q-value\n",
    "\n",
    "        # Charging logic (action = 0)\n",
    "        if action == 0:  # Charge the battery\n",
    "            reward += -(electricity_price_per_unit[state[0]] * electricity_demand[state[0]])  # Cost of charging\n",
    "            state[1] += solar_power_generation[state[0]]  # Add solar power to the battery\n",
    "        else:  # Discharge the battery (action = 1)\n",
    "            if state[1] >= electricity_demand[state[0]]:  # If there's enough battery power\n",
    "                reward += 0  # No additional cost for discharging\n",
    "                state[1] -= electricity_demand[state[0]]  # Discharge battery\n",
    "            else:  # Not enough battery power to discharge\n",
    "                # If not enough energy, the agent will need to buy extra electricity from the grid\n",
    "                reward -= (electricity_price_per_unit[state[0]] * (electricity_demand[state[0]] - state[1]))  # Cost of extra electricity\n",
    "                state[1] = 0  # Battery is empty after discharge\n",
    "\n",
    "        # Ensure battery level is within valid range (0 to max_battery_capacity)\n",
    "        state[1] = np.clip(state[1], 0, max_battery)\n",
    "        \n",
    "        # Store experience in replay buffer\n",
    "        replay_buffer.store(state, action, reward, state)\n",
    "\n",
    "        # Increment the hour\n",
    "        state[0] += 1  # Move to the next hour\n",
    "\n",
    "        # Perform experience replay\n",
    "        if replay_buffer.size() >= batch_size:\n",
    "            minibatch = replay_buffer.sample(batch_size)\n",
    "            states_batch = np.array([item[0] for item in minibatch])\n",
    "            actions_batch = np.array([item[1] for item in minibatch])\n",
    "            rewards_batch = np.array([item[2] for item in minibatch])\n",
    "            next_states_batch = np.array([item[3] for item in minibatch])\n",
    "            update_q_values(states_batch, actions_batch, rewards_batch, next_states_batch)\n",
    "\n",
    "    # Epsilon decay\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Periodic target network update (every two episode)\n",
    "    if episode % 2 == 0:\n",
    "        update_target_network()\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Reward: {reward}, Epsilon: {epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec326a-8579-4ac7-b1aa-1b657d648245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
