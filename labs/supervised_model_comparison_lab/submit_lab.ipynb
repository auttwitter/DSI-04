{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Supervised Learning Model Comparison\n",
    "\n",
    "---\n",
    "\n",
    "### Let us begin...\n",
    "\n",
    "Recall the `data science process`.\n",
    "   1. Define the problem.\n",
    "   2. Gather the data.\n",
    "   3. Explore the data.\n",
    "   4. Model the data.\n",
    "   5. Evaluate the model.\n",
    "   6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you.\n",
    "Most of the questions requiring a written response can be written in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "#### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. \n",
    "\n",
    "#### When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, mean_squared_error\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor, BaggingClassifier, \\\n",
    "RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "np.random.seed(42)   # seed for reproduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('401ksubs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e401k       int64\n",
       "inc       float64\n",
       "marr        int64\n",
       "male        int64\n",
       "age         int64\n",
       "fsize       int64\n",
       "nettfa    float64\n",
       "p401k       int64\n",
       "pira        int64\n",
       "incsq     float64\n",
       "agesq       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to predict the income, it may helpful if we have: \n",
    "# career category,\n",
    "# length of working (experience),\n",
    "# level of education\n",
    "# employment type (full-time, part-time, freelance, contract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The race is a sensitive data.\n",
    "# It may lead to discrimination and bias.\n",
    "# It may illegal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inc^2 (the square of income)\n",
    "# If we know this value, inc (income) can be find by the square root.\n",
    "\n",
    "# nettfa: net total fin. assets, $1000\n",
    "# It may be collinear with other variables, as nettfa is a measure of wealth \n",
    "# and resembles a target variable more than a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs (Subject Matter Experts) might have done this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agesq (the square of age): As a person gains experience and ages, \n",
    "# their career progresses, but income may not grow linearly. \n",
    "# Therefore, agesq is created to capture this non-linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, one variable description appears to be an error. What is this error, and what do you think the correct value would be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inc should be defined as income, not inc^2\n",
    "\n",
    "# e401k           byte   %9.0g                  =1 if eligble for 401(k)\n",
    "# inc             float  %9.0g                  inc^2\n",
    "# marr            byte   %9.0g                  =1 if married\n",
    "# male            byte   %9.0g                  =1 if male respondent\n",
    "# age             byte   %9.0g                  age^2\n",
    "# fsize           byte   %9.0g                  family size\n",
    "# nettfa          float  %9.0g                  net total fin. assets, $1000\n",
    "# p401k           byte   %9.0g                  =1 if participate in 401(k)\n",
    "# pira            byte   %9.0g                  =1 if have IRA\n",
    "# incsq           float  %9.0g                  inc^2\n",
    "# agesq           int    %9.0g                  age^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific regression problem and explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression - will work if the relationship looks linear\n",
    "# Logistic Regression - This is for classification problem\n",
    "# Decision Regressor Tree - If no need of coefficent for interpretability, it's ok \n",
    "# Gradient Boosting - Will try\n",
    "# Gradient Descent - Will try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above:\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the set of Pipeline for each model\n",
    "\n",
    "X = df[['marr','male','age','agesq','fsize']]\n",
    "y = df['inc']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X\n",
    "                                                    , y\n",
    "                                                    , test_size = 0.2\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7420.000000\n",
       "mean       39.243698\n",
       "std        24.163015\n",
       "min        10.008000\n",
       "25%        21.643500\n",
       "50%        33.241500\n",
       "75%        50.250000\n",
       "max       199.041000\n",
       "Name: inc, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['inc'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of Pipelines\n",
    "# We scale only continuous columns, not binary\n",
    "\n",
    "# List of continuous columns\n",
    "continuous_cols = ['age','agesq','fsize']\n",
    "\n",
    "# ColumnTransformer for Scale only continuous columns and leave others column untouch\n",
    "preprocessor = ColumnTransformer(transformers = [('scaler', StandardScaler(), continuous_cols)], remainder = 'passthrough')\n",
    "\n",
    "# Pipeline\n",
    "pipelines = { 'LR': Pipeline([('preprocessor', preprocessor ), ('regressor', LinearRegression())])\n",
    "             , 'KNN': Pipeline([('preprocessor', preprocessor ), ('regressor', KNeighborsRegressor())])\n",
    "             , 'DCT': Pipeline([('preprocessor', preprocessor ), ('regressor', DecisionTreeRegressor())])\n",
    "             , 'BAG': Pipeline([('preprocessor', preprocessor ), ('regressor', BaggingRegressor(DecisionTreeRegressor()))])\n",
    "             , 'RF': Pipeline([('preprocessor', preprocessor ), ('regressor', RandomForestRegressor())])\n",
    "             , 'ADA': Pipeline([('preprocessor', preprocessor ), ('regressor', AdaBoostRegressor())])\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'LR': {}\n",
    "    , 'KNN': {'regressor__n_neighbors': [1, 3, 5, 7, 9]}\n",
    "    , 'DCT': {'regressor__max_depth': [None, 10, 20, 30]}\n",
    "    , 'BAG': {'regressor__n_estimators': [10, 50, 100], 'regressor__random_state': [42]}\n",
    "    , 'RF': {'regressor__n_estimators': [10, 50, 100], 'regressor__max_depth': [None, 10, 20]}\n",
    "    , 'ADA': {'regressor__n_estimators': [10, 50, 100], 'regressor__learning_rate': [0.001, 0.01, 0.1, 1]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearch for LR...\n",
      "Best parameters for LR: {}\n",
      "Best score for LR: 21.69155441448897\n",
      "\n",
      "Running GridSearch for KNN...\n",
      "Best parameters for KNN: {'regressor__n_neighbors': 9}\n",
      "Best score for KNN: 22.6713908103253\n",
      "\n",
      "Running GridSearch for DCT...\n",
      "Best parameters for DCT: {'regressor__max_depth': 10}\n",
      "Best score for DCT: 22.40769851124346\n",
      "\n",
      "Running GridSearch for BAG...\n",
      "Best parameters for BAG: {'regressor__n_estimators': 100, 'regressor__random_state': 42}\n",
      "Best score for BAG: 22.512692300715177\n",
      "\n",
      "Running GridSearch for RF...\n",
      "Best parameters for RF: {'regressor__max_depth': 10, 'regressor__n_estimators': 50, 'regressor__random_state': 42}\n",
      "Best score for RF: 22.017088665535045\n",
      "\n",
      "Running GridSearch for ADA...\n",
      "Best parameters for ADA: {'regressor__learning_rate': 0.001, 'regressor__n_estimators': 50, 'regressor__random_state': 42}\n",
      "Best score for ADA: 21.804819237041198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each model and its parameter grid\n",
    "# Print each steps for checking\n",
    "# grid search has RMSE scoring in negative form 'neg_root_mean_squared_error'; we take absolute to get positive value\n",
    "\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    \n",
    "    print(f\"Running GridSearch for {model_name}...\")\n",
    "   \n",
    "    grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grids[model_name], cv = 5, scoring = 'neg_root_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "     \n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {model_name}: {abs(grid_search.best_score_)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a method that involves repeatedly resampling data with replacement, where each sample has the same size as the original. \n",
    "# Then, we aggregate the prediction values (by taking the mean) from each sample to obtain the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A decision tree is a single predictive model. It is fast to compute but tends to overfit.\n",
    "\n",
    "# A bagged decision tree creates multiple trees using bootstrapping. \n",
    "# The final result is an aggregation of predictions from all the trees, which helps reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every tree in bagged decision trees can use all features, which can lead to correlation among the trees. \n",
    "# On the other hand, in a random forest, each tree can only access a random subset of features, \n",
    "# which reduces correlation between trees. The final result is obtained by aggregating the predictions from all trees, \n",
    "# making the random forest less correlated than bagged decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From 11.\n",
    "# The correlation among trees in a random forest is lower than in a bagged decision tree model, \n",
    "# leading to less variance in the random forest. \n",
    "# However, because a random forest uses fewer features for each split, it may introduce some bias. \n",
    "# Still, this trade-off between reduced variance and slightly increased bias often results in better overall performance in a random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearch for LR...\n",
      "Best parameters for LR: {}\n",
      "Best RMSE score: 21.69155441448897\n",
      "Training RMSE for LR: 21.672411703221243\n",
      "Testing RMSE for LR: 22.417156974691792\n",
      "\n",
      "Running GridSearch for KNN...\n",
      "Best parameters for KNN: {'regressor__n_neighbors': 9}\n",
      "Best RMSE score: 22.6713908103253\n",
      "Training RMSE for KNN: 21.4750048255395\n",
      "Testing RMSE for KNN: 23.28834102056521\n",
      "\n",
      "Running GridSearch for DCT...\n",
      "Best parameters for DCT: {'regressor__max_depth': 10}\n",
      "Best RMSE score: 22.40704524073252\n",
      "Training RMSE for DCT: 20.91330712825238\n",
      "Testing RMSE for DCT: 22.718558013822282\n",
      "\n",
      "Running GridSearch for BAG...\n",
      "Best parameters for BAG: {'regressor__n_estimators': 100, 'regressor__random_state': 42}\n",
      "Best RMSE score: 22.512692300715177\n",
      "Training RMSE for BAG: 20.45619559842459\n",
      "Testing RMSE for BAG: 22.9823100539742\n",
      "\n",
      "Running GridSearch for RF...\n",
      "Best parameters for RF: {'regressor__max_depth': 10, 'regressor__n_estimators': 50, 'regressor__random_state': 42}\n",
      "Best RMSE score: 22.017088665535045\n",
      "Training RMSE for RF: 20.753159487559113\n",
      "Testing RMSE for RF: 22.496087834569128\n",
      "\n",
      "Running GridSearch for ADA...\n",
      "Best parameters for ADA: {'regressor__learning_rate': 0.001, 'regressor__n_estimators': 50, 'regressor__random_state': 42}\n",
      "Best RMSE score: 21.804819237041198\n",
      "Training RMSE for ADA: 21.711281432553253\n",
      "Testing RMSE for ADA: 22.461659392457566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same as 8, but add the part of get the best estimator and also print test score\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"Running GridSearch for {model_name}...\")\n",
    "   \n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid = param_grids[model_name], cv=5, scoring = 'neg_root_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_      # Best model from Grid Search\n",
    "\n",
    "    # Calculate RMSE for training data\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    \n",
    "    # Calculate RMSE for testing data\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best RMSE score: {abs(grid_search.best_score_)}\")\n",
    "    print(f\"Training RMSE for {model_name}: {train_rmse}\")\n",
    "    print(f\"Testing RMSE for {model_name}: {test_rmse}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression and ADA have a smaller difference in RMSE between training and testing, \n",
    "# while the others show a significant difference, which suggests overfitting.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would choose linear regression because it has the best score among the other models. \n",
    "# Additionally, linear regression is simple and easy to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Feature Engineering: I've noticed that when using R^2 as the scoring system,\n",
    "#    the scores seem to show no strong relationship (i.e., values below 0.2).\n",
    "#    This means the model didn't capture the patterns, so it may useful when doing the interaction between features.\n",
    "# 2) Hyperparameter Tuning: Maybe we didn't set the parameters to get the best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the person who participate e401k are most likely to not participate in p401k.\n",
    "# This leads to multicollinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6). For each tactic, identify whether it is or is not appropriate for solving this specific classification problem and explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# KNN\n",
    "# Decision Tree\n",
    "# Random Forests\n",
    "# Adaboost\n",
    "\n",
    "# All models are okay to help classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above:\n",
    "    - a logistic regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e401k           byte   %9.0g                  =1 if eligble for 401(k)\n",
    "# inc             float  %9.0g                  inc^2\n",
    "# marr            byte   %9.0g                  =1 if married\n",
    "# male            byte   %9.0g                  =1 if male respondent\n",
    "# age             byte   %9.0g                  age^2\n",
    "# fsize           byte   %9.0g                  family size\n",
    "# nettfa          float  %9.0g                  net total fin. assets, $1000\n",
    "# p401k           byte   %9.0g                  =1 if participate in 401(k)\n",
    "# pira            byte   %9.0g                  =1 if have IRA\n",
    "# incsq           float  %9.0g                  inc^2\n",
    "# agesq           int    %9.0g                  age^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearch for LR...\n",
      "Best parameters for LR: {'classifier__C': 100, 'classifier__max_iter': 100, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "Best score for LR: 0.655256064690027\n",
      "\n",
      "Running GridSearch for KNN...\n",
      "Best parameters for KNN: {'classifier__metric': 'manhattan', 'classifier__n_neighbors': 9}\n",
      "Best score for KNN: 0.6470350404312668\n",
      "\n",
      "Running GridSearch for DCT...\n",
      "Best parameters for DCT: {'classifier__max_depth': 10, 'classifier__min_samples_split': 10}\n",
      "Best score for DCT: 0.6520215633423181\n",
      "\n",
      "Running GridSearch for BAG...\n",
      "Best parameters for BAG: {'classifier__n_estimators': 100}\n",
      "Best score for BAG: 0.6557951482479785\n",
      "\n",
      "Running GridSearch for RF...\n",
      "Best parameters for RF: {'classifier__max_depth': 10, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 50}\n",
      "Best score for RF: 0.6838274932614554\n",
      "\n",
      "Running GridSearch for ADA...\n",
      "Best parameters for ADA: {'classifier__estimator': DecisionTreeClassifier(max_depth=5), 'classifier__learning_rate': 0.01, 'classifier__n_estimators': 50}\n",
      "Best score for ADA: 0.6880053908355794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ignore warning from Logistic Regression \n",
    "# FitFailedWarning for that penalty and solver are mismatch\n",
    "# UserWarning for divergent of model\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "warnings.filterwarnings('ignore', category = FitFailedWarning)\n",
    "warnings.filterwarnings('ignore', category = UserWarning)\n",
    "\n",
    "# Dictionary of Pipelines\n",
    "X = df.drop(columns=['e401k', 'p401k'])\n",
    "y = df['e401k']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X\n",
    "                                                    , y\n",
    "                                                    , test_size = 0.2\n",
    "                                                    , stratify = y\n",
    "                                                   )\n",
    "\n",
    "# List of continuous columns\n",
    "continuous_cols = ['age', 'agesq', 'fsize']\n",
    "\n",
    "# ColumnTransformer for scaling only continuous columns\n",
    "preprocessor = ColumnTransformer(transformers=[('scaler', StandardScaler(), continuous_cols)], remainder = 'passthrough')\n",
    "\n",
    "# Define pipelines\n",
    "pipelines = {'LR': Pipeline([('preprocessor', preprocessor), ('classifier', LogisticRegression())])\n",
    "             , 'KNN': Pipeline([('preprocessor', preprocessor), ('classifier', KNeighborsClassifier())])\n",
    "             , 'DCT': Pipeline([('preprocessor', preprocessor), ('classifier', DecisionTreeClassifier())])\n",
    "             , 'BAG': Pipeline([('preprocessor', preprocessor), ('classifier', BaggingClassifier(DecisionTreeClassifier()))])\n",
    "             , 'RF': Pipeline([('preprocessor', preprocessor), ('classifier', RandomForestClassifier())])\n",
    "             , 'ADA': Pipeline([('preprocessor', preprocessor), ('classifier', AdaBoostClassifier(algorithm = 'SAMME'))])\n",
    "}\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'LR': {'classifier__C': [0.01, 0.1, 1, 10, 100]\n",
    "           , 'classifier__penalty': [None,'l1', 'l2']\n",
    "           , 'classifier__solver': ['lbfgs','liblinear']\n",
    "           , 'classifier__max_iter': [100, 1000, 10000]\n",
    "          }\n",
    "    , 'KNN': {'classifier__n_neighbors': [1, 3, 5, 7, 9]\n",
    "              , 'classifier__metric': ['euclidean', 'manhattan']\n",
    "             }\n",
    "    , 'DCT': {'classifier__max_depth': [None, 10, 20, 30]\n",
    "              , 'classifier__min_samples_split': [2, 10]\n",
    "             }\n",
    "    , 'BAG': {'classifier__n_estimators': [10, 50, 100]\n",
    "             }\n",
    "    , 'RF': {'classifier__n_estimators': [10, 50, 100]\n",
    "             , 'classifier__max_depth': [None, 10, 20]\n",
    "             , 'classifier__min_samples_split': [2, 10]\n",
    "            }\n",
    "    , 'ADA': {'classifier__n_estimators': [10, 50, 100]\n",
    "              , 'classifier__learning_rate': [0.001, 0.01, 0.1, 1]\n",
    "              , 'classifier__estimator': [DecisionTreeClassifier(max_depth = 3), DecisionTreeClassifier(max_depth = 5)]\n",
    "             }\n",
    "}\n",
    "\n",
    "# Iterate through each model and its parameter grid\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"Running GridSearch for {model_name}...\")\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = pipeline, param_grid = param_grids[model_name], cv = 5, scoring = 'accuracy', n_jobs = -1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best score for {model_name}: {grid_search.best_score_}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e401k\n",
       "0    0.608086\n",
       "1    0.391914\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# False Positive\n",
    "# Predict person is eligible, but actually preson is not eligible\n",
    "# False Negative\n",
    "# Predict person is not eligible, but actually person is eligible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# We would minimize false negative.\n",
    "# People who are not identified as eligible may risk not meeting their retirement goals, \n",
    "# which could lead to other serious problems, such as homelessness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for the answer you provided in problem 21. Which metric would we optimize in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Recall = TP / (TP + FN)\n",
    "# If we reduce FN, the recall will tend to 1 (perfect)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1-score is the harmonic mean of Precision and Recall\n",
    "# Precision = TP / (TP + FP)  and Recall = TP / (TP + FN)\n",
    "\n",
    "# This score will balance between false positive and false negative\n",
    "# If FP and FN are low,, the f1-score will tend to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearch for LR...\n",
      "Best parameters for LR: {'classifier__C': 100, 'classifier__max_iter': 100, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "Best score for LR: 0.655256064690027\n",
      "F1-score for LR on training set: 0.47297577138123836\n",
      "F1-score for LR on test set: 0.44966442953020136\n",
      "\n",
      "Running GridSearch for KNN...\n",
      "Best parameters for KNN: {'classifier__metric': 'manhattan', 'classifier__n_neighbors': 9}\n",
      "Best score for KNN: 0.6470350404312668\n",
      "F1-score for KNN on training set: 0.6005719733079123\n",
      "F1-score for KNN on test set: 0.4839467501957713\n",
      "\n",
      "Running GridSearch for DCT...\n",
      "Best parameters for DCT: {'classifier__max_depth': 10, 'classifier__min_samples_split': 10}\n",
      "Best score for DCT: 0.6498652291105121\n",
      "F1-score for DCT on training set: 0.668141592920354\n",
      "F1-score for DCT on test set: 0.5362210604929052\n",
      "\n",
      "Running GridSearch for BAG...\n",
      "Best parameters for BAG: {'classifier__n_estimators': 100}\n",
      "Best score for BAG: 0.65633423180593\n",
      "F1-score for BAG on training set: 1.0\n",
      "F1-score for BAG on test set: 0.5072796934865901\n",
      "\n",
      "Running GridSearch for RF...\n",
      "Best parameters for RF: {'classifier__max_depth': 10, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 50}\n",
      "Best score for RF: 0.6826145552560646\n",
      "F1-score for RF on training set: 0.7291783642148606\n",
      "F1-score for RF on test set: 0.5271317829457365\n",
      "\n",
      "Running GridSearch for ADA...\n",
      "Best parameters for ADA: {'classifier__estimator': DecisionTreeClassifier(max_depth=5), 'classifier__learning_rate': 0.01, 'classifier__n_estimators': 50}\n",
      "Best score for ADA: 0.6880053908355794\n",
      "F1-score for ADA on training set: 0.5693232499515222\n",
      "F1-score for ADA on test set: 0.5339652448657188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Iterate through each model and its parameter grid\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"Running GridSearch for {model_name}...\")\n",
    "\n",
    "    # Perform GridSearchCV to find the best parameters\n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grids[model_name], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best parameters and score for the current model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    # Print the best parameters and score\n",
    "    print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "    print(f\"Best score for {model_name}: {best_score}\")\n",
    "\n",
    "    # Predict on both the training set and the test set\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Calculate F1-score for the training set\n",
    "    f1_train = f1_score(y_train, y_train_pred)\n",
    "    print(f\"F1-score for {model_name} on training set: {f1_train}\")\n",
    "\n",
    "    # Calculate F1-score for the test set\n",
    "    f1_test = f1_score(y_test, y_test_pred)\n",
    "    print(f\"F1-score for {model_name} on test set: {f1_test}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# BAG is perfect performing on training set (f1-score = 1 means no false positive and false negative)\n",
    "# but the f1-score of testing set is only 0.50. This means it is overfit.\n",
    "\n",
    "# Other models seem overfitting too, except ADA and Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# From 25, the models that are not overfit is ADA and Logistic Regression\n",
    "# But, the score of training set of Logistic Regression is lower that ADA; there is some bias here.\n",
    "\n",
    "# Thus, our chosen model is ADA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1. Hyperparameter Tuning\n",
    "# 2. Feature Engineering\n",
    "# 3. Try another ensemble technique like GB, XGB, or stacking\n",
    "# 4. SMOTE, but I think it no need here, since proportion of postive class and negative class is 40/60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem.\n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Regression\n",
    "# At first, we try to use R^2 as scoring method; but the results are worst, all scores are less than 0.2\n",
    "# We think all original features are not work. Maybe we must do some feature engineering before doing the model.\n",
    "\n",
    "# From the below cell, when we check feature importance of ADA\n",
    "# The key feature for getting e401k is 'pira' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature  Importance\n",
      "6    pira    0.655679\n",
      "8   agesq    0.207367\n",
      "7   incsq    0.059440\n",
      "3     age    0.034935\n",
      "0     inc    0.034444\n",
      "2    male    0.008135\n",
      "1    marr    0.000000\n",
      "4   fsize    0.000000\n",
      "5  nettfa    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Best ADA parameter get from grid search of Q19\n",
    "best_ada = Pipeline([('preprocessor', preprocessor)\n",
    "                     , ('classifier', AdaBoostClassifier(estimator = DecisionTreeClassifier(max_depth=5)\n",
    "                                                         , learning_rate = 0.01\n",
    "                                                         , n_estimators = 50\n",
    "                                                         , algorithm = 'SAMME'\n",
    "                                                        )\n",
    "                       )\n",
    "                    ])\n",
    "\n",
    "best_ada.fit(X_train, y_train)    # fit\n",
    "\n",
    "base_estimators = best_ada.named_steps['classifier'].estimators_\n",
    "feature_importance = base_estimators[0].feature_importances_\n",
    "\n",
    "# Show feature importancce\n",
    "\n",
    "feature_names = X_train.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names\n",
    "                              , 'Importance': feature_importance\n",
    "                             }).sort_values(by = 'Importance', ascending = False)\n",
    "\n",
    "print(importance_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
